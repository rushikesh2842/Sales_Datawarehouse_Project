{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		},
		"toc-autonumbering": true
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |",
			"metadata": {
				"deletable": false,
				"editable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n",
			"metadata": {
				"trusted": true,
				"editable": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: fcb82baf-9639-45c4-affa-2787177aa1dd\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\nWaiting for session fcb82baf-9639-45c4-affa-2787177aa1dd to get into ready status...\nSession fcb82baf-9639-45c4-affa-2787177aa1dd has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## What is RDD (Resilient Distributed Dataset)?",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. \nImmutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. ",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Creating RDD",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "RDDs are created primarily in two different ways,\n\n* parallelizing an existing collection and\n* referencing a dataset in an external storage system (HDFS, S3 and many more). ",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "#### Create RDD from parallelize ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#Create RDD from parallelize    \n\ndata = [1,2,3,4,5,6,7,8,9,10,11,12]\n\nrdd=spark.sparkContext.parallelize(data)\n",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "type(rdd)",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "<class 'pyspark.rdd.RDD'>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "rdd.count()",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "12\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Creating RDD referencing a dataset in S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Retrieve the list of existing buckets\n\nimport boto3\n\ns3 = boto3.client('s3')\nresponse = s3.list_buckets()\n\n# Output the bucket names\nprint('Existing buckets:')\nfor bucket in response['Buckets']:\n    print(f'  {bucket[\"Name\"]}')",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Existing buckets:\n  aws-glue-assets-030798167757-us-east-1\n  aws-glue-bootcamp-030798167757-us-east-1\n  bluejeans59\n  cdk-hnb659fds-assets-030798167757-us-east-1\n  cf-templates-leyxvlmxcxmr-us-east-1\n  deh-awsbootcamp-030798167757-us-east-1\n  deh-awsdataengineering-030798167757-us-east-1\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n\n## rdd = spark.sparkContext.textFile(\"s3://${BUCKET_NAME}/input/lab2/sample.csv\")\n\nrdd = spark.sparkContext.textFile(\"s3://aws-glue-bootcamp-030798167757-us-east-1/raw/sales/input/sales_rds_excercise.csv\")",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### RDD Actions ",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "##### count() --> Returns the number of records in an RDD",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "\n# Action - count\n\nprint(\"Count : \"+str(rdd.count()))\n",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "Count : 900\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "##### first() --> Returns the first record.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "\n# Action - first\n\nfirstRec = rdd.first()\n\nprint(\"First Record : \"+ firstRec)\n",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "First Record : uuid,Country,ItemType,SalesChannel,OrderPriority,OrderDate,Region,ShipDate,UnitsSold,UnitPrice,UnitCost,TotalRevenue,TotalCost,TotalProfit\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## By using rdd.first(), we have identified the spark RDD rdd which you have just create has first row as header. In this example you are loooking to filter the row as header.\n\nrddheader = rdd.first()\n\nrddwithoutheader = rdd.filter(lambda line: line != rddheader)\nrddwithoutheader.take(2)",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "['874708545,Panama,Cosmetics,Offline,L,2/22/2015,Central America and the Caribbean,2/27/2015,4551,437.2,263.33,1989697.2,1198414.83,791282.37', '854349935,Sao Tome and Principe,Fruits,Offline,M,12/9/2015,Sub-Saharan Africa,1/18/2016,9986,9.33,6.92,93169.38,69103.12,24066.26']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Split the data into rows\nrdd = rddwithoutheader.map(lambda line: line.split(\",\"))\nrdd.take(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "[['874708545', 'Panama', 'Cosmetics', 'Offline', 'L', '2/22/2015', 'Central America and the Caribbean', '2/27/2015', '4551', '437.2', '263.33', '1989697.2', '1198414.83', '791282.37'], ['854349935', 'Sao Tome and Principe', 'Fruits', 'Offline', 'M', '12/9/2015', 'Sub-Saharan Africa', '1/18/2016', '9986', '9.33', '6.92', '93169.38', '69103.12', '24066.26']]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Create a tuple of (Region, (UnitsSold, TotalRevenue, TotalCost, TotalProfit))\ngrouped_data = rdd.map(lambda row: (row[6], (int(row[8]), float(row[11]), float(row[12]), float(row[13]))))\n\ngrouped_data.take(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "[('Middle East and North Africa', (934, 142509.72, 91008.96, 51500.76)), ('Central America and the Caribbean', (4551, 1989697.2, 1198414.83, 791282.37))]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Group data by region and sum the aggregations\nregion_aggregates = grouped_data.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3]))\nregion_aggregates.take(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "[('Australia and Oceania', (317097, 77734113.97, 54654560.129999995, 23079553.84)), ('Sub-Saharan Africa', (1128765, 285499363.1299999, 199361113.02000007, 86138250.11000001))]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n# Optionally, collect the results as a list for immediate display\nresults = region_aggregates.collect()\nfor region, (unitsSold, totalRevenue, totalCost, totalProfit) in results:\n print(f\"Region: {region}, Units Sold: {unitsSold}, Total Revenue: ${totalRevenue:.2f}, Total Cost: ${totalCost:.2f}, Total Profit: ${totalProfit:.2f}\")\n\n# Keep the RDD for further processing or saving\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "Region: Australia and Oceania, Units Sold: 317097, Total Revenue: $77734113.97, Total Cost: $54654560.13, Total Profit: $23079553.84\nRegion: Sub-Saharan Africa, Units Sold: 1128765, Total Revenue: $285499363.13, Total Cost: $199361113.02, Total Profit: $86138250.11\nRegion: Middle East and North Africa, Units Sold: 563371, Total Revenue: $152260199.03, Total Cost: $109301663.24, Total Profit: $42958535.79\nRegion: Central America and the Caribbean, Units Sold: 448709, Total Revenue: $136074348.13, Total Cost: $95515203.56, Total Profit: $40559144.57\nRegion: Asia, Units Sold: 587833, Total Revenue: $168269165.41, Total Cost: $119429549.32, Total Profit: $48839616.09\nRegion: Europe, Units Sold: 1242668, Total Revenue: $366587296.27, Total Cost: $266285239.92, Total Profit: $100302056.35\nRegion: North America, Units Sold: 90173, Total Revenue: $15429562.63, Total Cost: $10437286.35, Total Profit: $4992276.28\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Saving the data to S3",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "**saveAsTextFile(path)**: Write the elements of the dataset as a text file (or set of text files) to a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "## save the value to s3 as text/CSV file with Gzip compression, replace the bucket name\n\nsc._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\", \"org.apache.hadoop.mapred.DirectFileOutputCommitter\")\ncodec = \"org.apache.hadoop.io.compress.GzipCodec\"\n\nregion_aggregates.saveAsTextFile(\"s3://aws-glue-bootcamp-030798167757-us-east-1/raw/sales/output\" + \"/part-0000*\" , codec)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Converting to DataFrame from RDD",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "In next section of this lab; we are working with DataFrames. Let's understand how to can convert RDD into dataframe. The easiest way is to use **rdd.toDF()** function.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# run one by one\ntype(region_aggregates)\n\nregion_aggregatesDF = region_aggregates.toDF()\n\ntype(region_aggregatesDF)\n",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "<class 'pyspark.sql.dataframe.DataFrame'>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "region_aggregatesDF.show(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------------------+\n|                  _1|                  _2|\n+--------------------+--------------------+\n|Australia and Oce...|{317097, 7.773411...|\n|  Sub-Saharan Africa|{1128765, 2.85499...|\n+--------------------+--------------------+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Since RDD is schema-less without column names and data type, converting from RDD to DataFrame gives you default column names as _1, _2 and so on and data type as String.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "region_aggregatesDF.printSchema()",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- _1: string (nullable = true)\n |-- _2: struct (nullable = true)\n |    |-- _1: long (nullable = true)\n |    |-- _2: double (nullable = true)\n |    |-- _3: double (nullable = true)\n |    |-- _4: double (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col\n\n# Flatten the struct column\nflattened_df = region_aggregatesDF.select(col(\"_1\").alias(\"Region\"),\n                                          col(\"_2._1\").alias(\"UnitsSold\"),\n                                          col(\"_2._2\").alias(\"TotalRevenue\"),\n                                          col(\"_2._3\").alias(\"TotalCost\"),\n                                          col(\"_2._4\").alias(\"TotalProfit\"))\n\n# Show the dataframe\nflattened_df.show(2)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+---------+-------------------+--------------------+-------------------+\n|              Region|UnitsSold|       TotalRevenue|           TotalCost|        TotalProfit|\n+--------------------+---------+-------------------+--------------------+-------------------+\n|Australia and Oce...|   317097|      7.773411397E7|5.4654560129999995E7|      2.307955384E7|\n|  Sub-Saharan Africa|  1128765|2.854993631299999E8|1.9936111302000007E8|8.613825011000001E7|\n+--------------------+---------+-------------------+--------------------+-------------------+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "Using createDataFrame() from SparkSession is another way to create and it takes rdd object as an argument. and chain with toDF() to specify names to the columns.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### Writing as Parquet to Amazon S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "flattened_df.write.parquet('s3://aws-glue-bootcamp-030798167757-us-east-1/raw/sales/parquetresult/')",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "## stop the current session \n\n%stop_session",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: fcb82baf-9639-45c4-affa-2787177aa1dd\nStopped session.\n",
					"output_type": "stream"
				}
			]
		}
	]
}